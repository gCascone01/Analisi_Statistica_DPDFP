---
title: "SAD Project"
author: "Cascone Giovanni, Vitale Ciro"
date: "2023-12-06"
output:
  html_document:
    toc: true
    theme: united
  pdf_document: default
---

```{r setup, include=FALSE}
# install.packages('readxl')
library(readxl)
# install.packages("ggplot2")
library(ggplot2)
# install.packages("gridExtra")
# library(gridExtra)

#---- DEFINIZIONE DEI PATH ----

pathGitProject_Gio = "C:/Users/user/Desktop/Magistrale/Statistica e Analisi dei Dati/SAD_Project"
setwd(pathGitProject_Gio)
source("Functions.R")

# pathGitProject_Ciro = "C:/Users/UTENTE/git/SAD_Project"
# setwd(pathGitProject_Ciro)
# source("Functions.R")

#---- CARICO I DATASET ----

n_countries = 27

mydata = read_xlsx('./Datasets/European-Country/Complete_Dataset.xlsx', 1)
data = as.matrix(mydata)

mydata15 = read_xlsx('./Datasets/European-Country/less15.xlsx', 1)
data15 = as.matrix(mydata15)

mydataComp = read_xlsx('./Datasets/European-Country/15-64.xlsx', 1)
dataComp = as.matrix(mydataComp)

mydata64 = read_xlsx('./Datasets/European-Country/64.xlsx', 1)
data64 = as.matrix(mydata64)

#---- DATASET to MATRICI ----

dataset = matrix( as.double(matrix(data[,-1],nrow=n_countries)) , nrow=n_countries)

dataset15 = matrix( as.double(matrix(data15[,-1],nrow=n_countries)) , nrow=n_countries)

datasetComp = matrix( as.double(matrix(dataComp[,-1],nrow=n_countries)) , nrow=n_countries)

dataset64 = matrix( as.double(matrix(data64[,-1],nrow=n_countries)) , nrow=n_countries)

#---- PREPARAZIONE dataset ----

#Rimozione anni che non analizziamo

dataset=dataset[,-c(1:10,49:50)]

dataset15=dataset15[,-c(1:10,49:50)]

datasetComp=datasetComp[,-c(1:10,49:50)]

dataset64=dataset64[,-c(1:10,49:50)]

#Definizione nomi righe con nomi nazioni

countries = c("Austria", "Belgio", "Bulgaria", "Cipro", "Croazia", "Danimarca", "Estonia", "Finlandia", "Francia", "Germania", "Grecia", "Ungheria", "Irlanda", "Italia", "Lettonia", "Lituania", "Lussemburgo", "Malta", "Paesi Bassi", "Polonia", "Portogallo", "Rep. Ceca", "Romania", "Slovacchia", "Slovenia", "Spagna", "Svezia")
rownames(dataset) = countries
rownames(dataset15) = countries
rownames(datasetComp) = countries
rownames(dataset64) = countries

# Variabili (DPM (D) e VSL (D)) in DATASET SEPARATI

D_Index = seq(1,48, by=2) # DEFINISCO GLI INDICI DELLE COLONNE PER OGNI VARIABILE
V_Index = seq(2,49, by=2) # D sta per Death, V per Value

D_dataset = dataset[, D_Index]
V_dataset = dataset[, V_Index]

D_dataset15 = dataset15[, D_Index]
V_dataset15 = dataset15[, V_Index]

D_datasetComp = datasetComp[, D_Index]
V_datasetComp = datasetComp[, V_Index]

D_dataset64 = dataset64[, D_Index]
V_dataset64 = dataset64[, V_Index]

years = seq(1995,2018)
colnames(D_dataset) = years
colnames(V_dataset) = years

colnames(D_dataset15) = years
colnames(V_dataset15) = years

colnames(D_datasetComp) = years
colnames(V_datasetComp) = years

colnames(D_dataset64) = years
colnames(V_dataset64) = years
```

# Caricamento Librerie

Librerie utilizzate: readxl, ggplot2.

# Caricamento Dataset e Preparazione

Caricamento dei dataset contenenti le variabili DPM e VSL per `r n_countries` paesi, suddivisi in fasce di età (minori di 15 anni, compresi tra 15 e 64 anni, maggiori di 64 anni). Preparazione dei dati: rimozione e formattazione righe e colonne, separazione delle variabili in dataset distinti.

# Statistica Descrittiva

```{r indici di centralità, echo=FALSE}
#---- CALCOLO INDICI CENTRALITÀ ----

D_min = min(D_dataset)
D_min15 = min(D_dataset15)
D_minComp = min(D_datasetComp)
D_min64 = min(D_dataset64)

V_min = min(V_dataset)
V_min15 = min(V_dataset15)
V_minComp = min(V_datasetComp)
V_min64 = min(V_dataset64)


D_max = max(D_dataset)
D_max15 = max(D_dataset15)
D_maxComp = max(D_datasetComp)
D_max64 = max(D_dataset64)

V_max = max(V_dataset)
V_max15 = max(V_dataset15)
V_maxComp = max(V_datasetComp)
V_max64 = max(V_dataset64)


D_mean = mean(D_dataset)
D_mean15 = mean(D_dataset15)
D_meanComp = mean(D_datasetComp)
D_mean64 = mean(D_dataset64)

# Media aritmetica
V_mean = mean(V_dataset)
V_mean15 = mean(V_dataset15)
V_meanComp = mean(V_datasetComp)
V_mean64 = mean(V_dataset64)


D_median = median(D_dataset)
D_median15 = median(D_dataset15)
D_medianComp = median(D_datasetComp)
D_median64 = median(D_dataset64)

V_median = median(V_dataset)
V_median15 = median(V_dataset15)
V_medianComp = median(V_datasetComp)
V_median64 = median(V_dataset64)



D_indiciCentr = rbind(c(D_min,D_max,D_mean,D_median),c(D_min15,D_max15,D_mean15,D_median15),c(D_minComp,D_maxComp,D_meanComp,D_medianComp),c(D_min64,D_max64,D_mean64,D_median64))
colnames(D_indiciCentr) = c("MIN", "MAX", "MEDIA", "MEDIANA")
rownames(D_indiciCentr) = c("ALL","LESS 15","COMP 15 - 64","GREATER 64")

V_indiciCentr = rbind(c(V_min,V_max,V_mean,V_median),c(V_min15,V_max15,V_mean15,V_median15),c(V_minComp,V_maxComp,V_meanComp,V_medianComp),c(V_min64,V_max64,V_mean64,V_median64))
colnames(V_indiciCentr) = c("MIN", "MAX", "MEDIA", "MEDIANA")
rownames(V_indiciCentr) = c("ALL","LESS 15","COMP 15 - 64","GREATER 64")
```
## Calcolo indici di Centralità (per dataset)

Di seguito il calcolo degli indici di centralità per i dataset suddivisi per eta e per variabili.

### Indici di Centralità per DPM

```{r view DPM indici di centralità, echo=FALSE}
D_indiciCentr
```

### Indici di Centralità per VSL

```{r view VSL indici di centralità, echo=FALSE}
V_indiciCentr
```

```{r indici di dispersione, echo=FALSE}
#---- CALCOLO INDICI DISPERSIONE ----

# Campo di Variazione

D_cdv = D_max - D_min
D_cdv15 = D_max15 - D_min15
D_cdvComp = D_maxComp - D_minComp
D_cdv64 = D_max64 - D_min64

V_cdv = V_max - V_min
V_cdv15 = V_max15 - V_min15
V_cdvComp = V_maxComp - V_minComp
V_cdv64 = V_max64 - V_min64

# Differenza interquartilica: meno sensibile a val estremi
D_diffInterq = unname(quantile(sort(D_dataset), probs = 0.75)) - unname(quantile(sort(D_dataset), probs = 0.25))
D_diffInterq15 = unname(quantile(sort(D_dataset15), probs = 0.75)) - unname(quantile(sort(D_dataset15), probs = 0.25))
D_diffInterqComp = unname(quantile(sort(D_datasetComp), probs = 0.75)) - unname(quantile(sort(D_datasetComp), probs = 0.25))
D_diffInterq64 = unname(quantile(sort(D_dataset64), probs = 0.75)) - unname(quantile(sort(D_dataset64), probs = 0.25))

V_diffInterq = unname(quantile(sort(V_dataset), probs = 0.75)) - unname(quantile(sort(V_dataset), probs = 0.25))
V_diffInterq15 = unname(quantile(sort(V_dataset15), probs = 0.75)) - unname(quantile(sort(V_dataset15), probs = 0.25))
V_diffInterqComp = unname(quantile(sort(V_datasetComp), probs = 0.75)) - unname(quantile(sort(V_datasetComp), probs = 0.25))
V_diffInterq64 = unname(quantile(sort(V_dataset64), probs = 0.75)) - unname(quantile(sort(V_dataset64), probs = 0.25))

# Varianza
D_var = var(as.vector(D_dataset))
D_var15 = var(as.vector(D_dataset15))
D_varComp = var(as.vector(D_datasetComp))
D_var64 = var(as.vector(D_dataset64))

V_var = var(as.vector(V_dataset))
V_var15 = var(as.vector(V_dataset15))
V_varComp = var(as.vector(V_datasetComp))
V_var64 = var(as.vector(V_dataset64))

# Varianza per colonne (Forse ha più senso farlo per righe, ragioniamoci bene) Cosa fare ?????
# D_var_col = apply(D_dataset, 2, var)
# V_var_col = apply(D_dataset, 2, var)

# Deviazione Standard totale: valore differisce dalla media aritmetica dei valori, in media quadratica di: x
D_sd = sd(D_dataset)
D_sd15 = sd(D_dataset15)
D_sdComp = sd(D_datasetComp)
D_sd64 = sd(D_dataset64)

V_sd = sd(V_dataset)
V_sd15 = sd(V_dataset15)
V_sdComp = sd(V_datasetComp)
V_sd64 = sd(V_dataset64)


D_indiciDisp = rbind(c(D_cdv,D_diffInterq,D_var,D_sd),c(D_cdv15,D_diffInterq15,D_var15,D_sd15),c(D_cdvComp,D_diffInterqComp,D_varComp,D_sdComp),c(D_cdv64,D_diffInterq64,D_var64,D_sd64))
colnames(D_indiciDisp) = c("CdV", "DIFF. INTERQ.", "VARIANZA", "DEVIAZ. STD")
rownames(D_indiciDisp) = c("ALL","LESS 15","COMP 15 - 64","GREATER 64")

V_indiciDisp = rbind(c(V_cdv,V_diffInterq,V_var,V_sd),c(V_cdv15,V_diffInterq15,V_var15,V_sd15),c(V_cdvComp,V_diffInterqComp,V_varComp,V_sdComp),c(V_cdv64,V_diffInterq64,V_var64,V_sd64))
colnames(V_indiciDisp) = c("CdV", "DIFF. INTERQ.", "VARIANZA", "DEVIAZ. STD")
rownames(V_indiciDisp) = c("ALL","LESS 15","COMP 15 - 64","GREATER 64")
```

## Calcolo indici di Dispersione (per dataset)

Di seguito il calcolo degli indici di dispersione per i dataset suddivisi per eta e per variabili.

### Indici di Dispersione per DPM

```{r view DPM indici di dispersione, echo=FALSE}
D_indiciDisp
```

### Indici di Dispersione per VSL

```{r view VSL indici di dispersione, echo=FALSE}
V_indiciDisp
```


## Quantili (per dataset)

```{r quantili, echo=FALSE}
par(mfrow=c(1, 2))

f_quantili(1,D_dataset, "DPM - ALL")
f_quantili(6,D_dataset, "DPM - ALL")
f_quantili(11,D_dataset, "DPM - ALL")
f_quantili(16,D_dataset, "DPM - ALL")
f_quantili(21,D_dataset, "DPM - ALL")

f_quantili(1,D_dataset15, "DPM - LESS 15")
f_quantili(6,D_dataset15, "DPM - LESS 15")
f_quantili(11,D_dataset15, "DPM - LESS 15")
f_quantili(16,D_dataset15, "DPM - LESS 15")
f_quantili(21,D_dataset15, "DPM - LESS 15")

f_quantili(1,D_datasetComp, "DPM - COMP 15 - 64")
f_quantili(6,D_datasetComp, "DPM - COMP 15 - 64")
f_quantili(16,D_datasetComp, "DPM - COMP 15 - 64")
f_quantili(21,D_datasetComp, "DPM - COMP 15 - 64")

f_quantili(1,D_dataset64, "DPM - GREATER 64")
f_quantili(6,D_dataset64, "DPM - GREATER 64")
f_quantili(11,D_dataset64, "DPM - GREATER 64")
f_quantili(16,D_dataset64, "DPM - GREATER 64")
f_quantili(21,D_dataset64, "DPM - GREATER 64")

f_quantili(1,V_dataset, "VSL - ALL")
f_quantili(6,V_dataset, "VSL - ALL")
f_quantili(11,V_dataset, "VSL - ALL")
f_quantili(16,V_dataset, "VSL - ALL")
f_quantili(21,V_dataset, "VSL - ALL")

par(mfrow=c(1, 1))
```


## Funzione di Distribuzione Empirica (per country)

```{r FdDC, echo=FALSE}
#---- FdDC - Funzione di Distribuzione Empirica Continua ----

par(mfrow=c(2, 2))

for(country in countries){
  f_FdDC(country)
}

par(mfrow=c(1, 1))

```

## Serie Temporali
La legenda è attualmente nascosta perchè non riesco a metterla a lato per via dei margini di R MARKDOWN !!!

```{r serie temporali, echo=FALSE}
f_timeSeries(D_dataset, "DPM ALL")
f_timeSeries(V_dataset, "VSL ALL")

f_timeSeries(D_dataset15, "DPM LESS 15")
f_timeSeries(D_datasetComp, "DPM COMP 15 - 64")
f_timeSeries(D_dataset64, "DPM GREATER 64")
```

### Grafico a Barre Sovrapposte per DPM (per country)

```{r grafico a barre sovrapposte DPM, echo=FALSE}

#par(mfrow=c(2,2))

#---- GRAFICO A BARRE SOVRAPPOSTE ----
for(country in countries){
  print(f_barre_sovrapp_per_eta(country,D_dataset15,D_datasetComp,D_dataset64,"DPM"))
}

#par(mfrow=c(1, 1))
```
## Regressione

### Grafici di regressione

<!-- capire se ci sono schemi nei grafici dei residui: -->

<!-- - LINEARE: NO -->

<!-- PREVISIONE DATI FUTURI??????? -->

<!-- ```{r grafii di regressione lineare, echo=FALSE} -->

<!-- #---- GRAFICI DI REGRESSIONE LINEARE ---- -->

<!-- # La regressione lineare è stata utilizzata per le variabili che presentano un coeff di correlazione < -0.85 e > 0.85 e per i modelli per cui risulta un coeff di determinazione > 0.7 -->

<!-- regression_linear_c_cor_res = numeric() -->
<!-- regression_linear_r_squared_res = numeric() -->
<!-- regression_linear_resid_res = numeric() -->
<!-- regression_non_linear_country = character() -->

<!-- regression_linear_country = character() -->
<!-- par(mfrow=c(1, 2)) -->
<!-- for(country in countries) { -->
<!--   results <- f_regressione_lineare(V_dataset[country,], D_dataset[country,], country, 0.7) -->

<!--   if(!is.null(results)) { -->
<!--     if(is.character(results)) { -->
<!--       regression_non_linear_country <- c(regression_non_linear_country, results) -->
<!--     } else { -->
<!--       regression_linear_country <- c(regression_linear_country, country) -->

<!--       regression_linear_c_cor_res <- c(regression_linear_c_cor_res, results[["c_cor"]]) -->
<!--       regression_linear_r_squared_res <- c(regression_linear_r_squared_res, results[["r_squared"]]) -->

<!--       regression_linear_resid_res <- c(regression_linear_resid_res, sum(results[["resid"]]^2)) -->
<!--     } -->
<!--   } -->
<!-- } -->
<!-- par(mfrow=c(1, 1)) -->


<!-- res_reg_lin <- data.frame( -->
<!--   cor = regression_linear_c_cor_res,  -->
<!--   r_squared = regression_linear_r_squared_res,  -->
<!--   resid = regression_linear_resid_res -->
<!-- ) -->

<!-- row.names(res_reg_lin) <- regression_linear_country -->

<!-- res_reg_lin -->
<!-- ``` -->

```{r grafici di regressione, echo=FALSE}

#---- GRAFICI DI REGRESSIONE ----


par(mfrow=c(1, 2))

reg_non_lin_best_model <- character()
reg_non_lin_best_r_squared <- numeric()
reg_non_lin_best_resid <- numeric()

summary_models <- data.frame(best_model=character(), c_cor=numeric(), lin = numeric(), quad = numeric(), exp = numeric(), semilog = numeric(), log = numeric(), stringsAsFactors = FALSE)

for(country in countries) {
  results <- best_model_function(V_dataset[country, ], D_dataset[country, ], country)
  
  reg_non_lin_best_model <- c(reg_non_lin_best_model, results$model)
  reg_non_lin_best_r_squared <- c(reg_non_lin_best_r_squared, results$r_squared)
  reg_non_lin_best_resid <- c(reg_non_lin_best_resid, results$resid)
  
  summary_models <- rbind(summary_models, results$summary)
}

row.names(summary_models) <- countries

par(mfrow=c(1, 1))

res_reg_non_lin <- data.frame(
  model = reg_non_lin_best_model,
  r_squared = reg_non_lin_best_r_squared, 
  resid = reg_non_lin_best_resid
)

# row.names(res_reg_non_lin) <- regression_non_linear_country

summary_models
```
## Clustering

```{r clustering setup VSL, echo=FALSE}

### DOMANDE

# algoritmo di enumerazione completa?
# scale anche se è una sola var?
# elbow point?

#---- SETUP ----

# MATRICE DELLE DISTANZE

V_distance <- dist(V_dataset, method="euclidean")

# MATRICE DI NON OMOGENEITÀ TOTALE

V_WI <- cov(V_dataset)

# MATRICE STATISTICA DI NON OMOGENEITÀ TOTALE

V_n <- length(V_dataset)

V_HI <- (V_n-1)*V_WI

# TRACCIA DI HI

V_trHI <- sum(diag(V_HI))

```

### Metodi gerarchici VSL

#### Metodo del legame singolo

```{r clustering metodi gerarchici - legame singolo VSL, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL LEGAME SINGOLO ----

num_clusters <- 2:8
hls <- hclust(V_distance, method="single")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("VSL - Metodo gerarchico agglomerativo"), sub=paste("del legame singolo"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

# Da 1 a 2 risulta il salto maggiore, quindi sembra esser il miglior, secondo questa procedura EMPIRICA, l'utilizzo di 2 cluster, c'è un ulteriore salto lungo anche da 2 a 3, quindi proveremo anche per 3, fino a 8.

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  V_trH_average_results <- hierarchClustering(hls, n_clust, V_dataset, "del legame singolo", V_trHI, "VSL")
  
  trH_within_values[i-1] <- V_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- V_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


# Sia graficamente, che analizzando le misure di non omogeneità between e within e guardando i grafici visualizzati precedentemente (quindi punto di gomito), il numero di cluster k=4 sembra il più adatto ai nostri dati



```

#### Metodo del legame completo

```{r clustering metodi gerarchici - legame completo VSL, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL LEGAME COMPLETO ----
num_clusters <- 2:8
hls <- hclust(V_distance, method="complete")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("VSL - Metodo gerarchico agglomerativo"), sub=paste("del legame completo"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

# Da 1 a 2 risulta il salto maggiore, quindi sembra esser il miglior, secondo questa procedura EMPIRICA, l'utilizzo di 2 cluster, c'è un ulteriore salto lungo anche da 2 a 3, quindi proveremo anche per 3, fino a 8.

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  V_trH_average_results <- hierarchClustering(hls, n_clust, V_dataset, "del legame completo", V_trHI, "VSL")
  
  trH_within_values[i-1] <- V_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- V_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


# Sia graficamente, che analizzando le misure di non omogeneità between e within e guardando i grafici visualizzati precedentemente (quindi punto di gomito), il numero di cluster k=4 sembra il più adatto ai nostri dati... Forse essendo il salto maggiore verso 3, questo metodo suggerisce k=4, ma non ne sono sicuro, son più confident di 4.



```

#### Metodo del legame medio

```{r clustering metodi gerarchici - legame medio VSL, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL LEGAME MEDIO ----
num_clusters <- 2:8
hls <- hclust(V_distance, method="average")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("VSL - Metodo gerarchico agglomerativo"), sub=paste("del legame medio"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

# Da 1 a 2 risulta il salto maggiore, quindi sembra esser il miglior, secondo questa procedura EMPIRICA, l'utilizzo di 2 cluster, c'è un ulteriore salto lungo anche da 2 a 3, quindi proveremo anche per 3, fino a 8.

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  V_trH_average_results <- hierarchClustering(hls, n_clust, V_dataset, "del legame medio", V_trHI, "VSL")
  
  trH_within_values[i-1] <- V_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- V_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


# Sia graficamente, che analizzando le misure di non omogeneità between e within e guardando i grafici visualizzati precedentemente (quindi punto di gomito), il numero di cluster k=4 sembra il più adatto ai nostri dati... Forse essendo il salto maggiore verso 3, questo metodo suggerisce k=4, ma non ne sono sicuro, son più confident di 4.



```

#### Metodo del centroide

```{r clustering metodi gerarchici - centroide VSL, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL CENTROIDE ----
num_clusters <- 2:8
V_distance2 <- V_distance^2
hls <- hclust(V_distance2, method="centroid")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("VSL - Metodo gerarchico agglomerativo"), sub=paste("del centroide"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

# Da 1 a 2 risulta il salto maggiore, quindi sembra esser il miglior, secondo questa procedura EMPIRICA, l'utilizzo di 2 cluster, c'è un ulteriore salto lungo anche da 2 a 3, quindi proveremo anche per 3, fino a 8.

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  V_trH_average_results <- hierarchClustering(hls, n_clust, V_dataset, "del centroide", V_trHI, "VSL")
  
  trH_within_values[i-1] <- V_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- V_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


# Sia graficamente, che analizzando le misure di non omogeneità between e within e guardando i grafici visualizzati precedentemente (quindi punto di gomito), il numero di cluster k=4 sembra il più adatto ai nostri dati... Forse essendo il salto maggiore verso 3, questo metodo suggerisce k=4, ma non ne sono sicuro, son più confident di 4.



```


#### Metodo della mediana

```{r clustering metodi gerarchici - mediana VSL, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DELLA MEDIANA ----
num_clusters <- 2:8
V_distance2 <- V_distance^2
hls <- hclust(V_distance2, method="median")

# Dendogramma

plot(hls, hang=-1, xlab=paste("VSL - Metodo gerarchico agglomerativo"), sub=paste("della mediana"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

# Da 1 a 2 risulta il salto maggiore, quindi sembra esser il miglior, secondo questa procedura EMPIRICA, l'utilizzo di 2 cluster, c'è un ulteriore salto lungo anche da 2 a 3, quindi proveremo anche per 3, fino a 8.

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  V_trH_average_results <- hierarchClustering(hls, n_clust, V_dataset, "della mediana", V_trHI, "VSL")
  
  trH_within_values[i-1] <- V_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- V_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


# Sia graficamente, che analizzando le misure di non omogeneità between e within e guardando i grafici visualizzati precedentemente (quindi punto di gomito), il numero di cluster k=4 sembra il più adatto ai nostri dati... Forse essendo il salto maggiore verso 3, questo metodo suggerisce k=4, ma non ne sono sicuro, son più confident di 4.



```

### Metodi non gerarchici VSL

#### Metodo kmeans

```{r clustering metodi non gerarchici - kmeans VSL, echo=FALSE}

#---- METODI NON GERARCHICI ----

#---- METODO KMEANS ----
n_clust = 3:5
for(i in n_clust){
  V_km <- kMeansClustering(V_dataset, i, 5, 10, "VSL")

  V_trH_km_within <- V_km[["trH_within"]]
  
  V_trH_km_between <- V_km[["trH_between"]]
  
  V_km_clusters <- V_km[["clusters"]]
}



# Sia graficamente, che analizzando le misure di non omogeneità between e within e guardando i grafici visualizzati precedentemente (quindi punto di gomito), il numero di cluster k=4 sembra il più adatto ai nostri dati... Forse essendo il salto maggiore verso 3, questo metodo suggerisce k=4, ma non ne sono sicuro, son più confident di 4.



```

```{r clustering setup DPM, echo=FALSE}

### DOMANDE

# algoritmo di enumerazione completa?

#---- SETUP ----

# MATRICE DELLE DISTANZE

D_distance <- dist(D_dataset, method="euclidean")

# MATRICE DI NON OMOGENEITÀ TOTALE

D_WI <- cov(D_dataset)

# MATRICE STATISTICA DI NON OMOGENEITÀ TOTALE

D_n <- length(D_dataset)

D_HI <- (D_n-1)*D_WI

# TRACCIA DI HI

D_trHI <- sum(diag(D_HI))

```

### Metodi gerarchici DPM

#### Metodo del legame singolo

```{r clustering metodi gerarchici - legame singolo DPM, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL LEGAME SINGOLO ----

num_clusters <- 2:10
hls <- hclust(D_distance, method="single")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("DPM - Metodo gerarchico agglomerativo"), sub=paste("del legame singolo"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

#

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  D_trH_average_results <- hierarchClustering(hls, n_clust, D_dataset, "del legame singolo", D_trHI, "DPM")
  
  trH_within_values[i-1] <- D_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- D_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


#



```

#### Metodo del legame completo

```{r clustering metodi gerarchici - legame completo DPM, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL LEGAME COMPLETO ----
num_clusters <- 2:10
hls <- hclust(D_distance, method="complete")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("DPM - Metodo gerarchico agglomerativo"), sub=paste("del legame completo"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

#

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  D_trH_average_results <- hierarchClustering(hls, n_clust, D_dataset, "del legame completo", D_trHI, "DPM")
  
  trH_within_values[i-1] <- D_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- D_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


#



```

#### Metodo del legame medio

```{r clustering metodi gerarchici - legame medio DPM, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL LEGAME MEDIO ----
num_clusters <- 2:10
hls <- hclust(D_distance, method="average")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("DPM - Metodo gerarchico agglomerativo"), sub=paste("del legame medio"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

#

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  D_trH_average_results <- hierarchClustering(hls, n_clust, D_dataset, "del legame medio", D_trHI, "DPM")
  
  trH_within_values[i-1] <- D_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- D_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


# 



```

#### Metodo del centroide

```{r clustering metodi gerarchici - centroide DPM, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DEL CENTROIDE ----
num_clusters <- 2:10
D_distance2 <- D_distance^2
hls <- hclust(D_distance2, method="centroid")

# Dendogramma
  
plot(hls, hang=-1, xlab=paste("DPM - Metodo gerarchico agglomerativo"), sub=paste("del centroide"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

#

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  D_trH_average_results <- hierarchClustering(hls, n_clust, D_dataset, "del centroide", D_trHI, "DPM")
  
  trH_within_values[i-1] <- D_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- D_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


#


```


#### Metodo della mediana

```{r clustering metodi gerarchici - mediana DPM, echo=FALSE}

#---- METODI GERARCHICI ----

#---- METODO DELLA MEDIANA ----
num_clusters <- 2:10
D_distance2 <- D_distance^2
hls <- hclust(D_distance2, method="median")

# Dendogramma

plot(hls, hang=-1, xlab=paste("DPM - Metodo gerarchico agglomerativo"), sub=paste("della mediana"))
axis(side=4, at=round(c(0, hls$height), 2))

# Screeplot

plot(c(0, hls$height), seq(n_countries,1), type="b", main="Screeplot", xlab="Distanza di aggregazione", ylab="Numero di cluster", col="red")

#

trH_within_values <- numeric(length(num_clusters))
trH_between_values <- numeric(length(num_clusters))

for(i in num_clusters){
  n_clust <- i
  D_trH_average_results <- hierarchClustering(hls, n_clust, D_dataset, "della mediana", D_trHI, "DPM")
  
  trH_within_values[i-1] <- D_trH_average_results[["trH_within"]]
  trH_between_values[i-1] <- D_trH_average_results[["trH_between"]]
}

plot(num_clusters, trH_within_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_within)", main="Analisi del Numero Ottimale di Cluster")
plot(num_clusters, trH_between_values, type="b", xlab="Numero di Cluster", ylab="Non Omogeneità (trH_between)", main="Analisi del Numero Ottimale di Cluster")


#


```

### Metodi non gerarchici DPM

#### Metodo kmeans

```{r clustering metodi non gerarchici - kmeans DPM, echo=FALSE}

#---- METODI NON GERARCHICI ----

#---- METODO KMEANS ----
n_clust = 6:8
for(i in n_clust){
  D_km <- kMeansClustering(D_dataset, i, 5, 10, "DPM")

  D_trH_km_within <- D_km[["trH_within"]]
  
  D_trH_km_between <- D_km[["trH_between"]]
  
  D_km_clusters <- D_km[["clusters"]]
}



# n_clust = 4
# D_km <- kMeansClustering(D_dataset, n_clust, 5, 10)
# 
# D_trH_km_within <- D_km[["trH_within"]]
# 
# D_trH_km_between <- D_km[["trH_between"]]
# 
# D_km_clusters <- D_km[["clusters"]]
# 
# 
# n_clust = 5
# D_km <- kMeansClustering(D_dataset, n_clust, 5, 10)
# 
# D_trH_km_within <- D_km[["trH_within"]]
# 
# D_trH_km_between <- D_km[["trH_between"]]
# 
# D_km_clusters <- D_km[["clusters"]]



#



```
## Parte finale

### Chi2

Dato l'output:

Il valore calcolato del test chi quadrato (chi2) è 4.75.
Il quantile inferiore della distribuzione chi quadrato è 0.05063562.
Il quantile superiore della distribuzione chi quadrato è 7.377759.

Poiché il valore calcolato di 4.75 è compreso tra i due quantili (0.05063562 e 7.377759), non rifiutiamo l'ipotesi nulla. In altre parole, non abbiamo prove sufficienti per affermare che i dati non seguano la distribuzione normale specificata.

In sintesi, sulla base di questo test, i dati delle rilevazioni di DPM per l'Italia non mostrano significative deviazioni dalla normalità, secondo il livello di significatività scelto (α = 0.05).
```{r metodo chi quadrato, echo=FALSE}


# results <- apply(D_dataset, 2, function(column) {
#     # Definire le categorie (bins)
#     bins <- seq(min(column), max(column), length.out = 10)
#     
#     # Calcolare le frequenze osservate
#     observed <- hist(column, breaks = bins, plot = FALSE)$counts
#     
#     # Calcolare le frequenze attese
#     expected <- dnorm(bins, mean = mean(column), sd = sd(column)) * length(column) * diff(bins[1:2])
#     
#     # Eseguire il test del chi quadrato
#     chisq.test(x = observed, p = expected, rescale.p = TRUE)
# })
# 
# # Visualizzare i risultati
# print(results)

D_Italia <- as.vector(D_dataset["Italia",])
n_Italia = length(D_Italia)

mean_Italia <- mean(D_Italia)

sd_Italia <- sd(D_Italia)

a <- numeric(4)

for(i in 1:4){
  a[i] <- qnorm(0.2*i, mean=mean_Italia, sd=sd_Italia)
}

r <- 5

nint <- numeric(r)
nint [1] <- length(which(D_Italia < a [1]) )
for(i in 2:4) {
  nint[i] <- length(which((D_Italia >= a[i-1]) & D_Italia < a[i]))
}
nint[5] <- length ( which ( D_Italia >= a [4]) )

chi2 <- sum ((( nint - n_Italia* 0.2) / sqrt ( n_Italia* 0.2) ) ^2)

# r num di elementi per ogni intervallo di frequenze (per ogni cella)
r <-5
# k num di parametri non noti
k <-2
# r - k -1 gradi di libertà
alpha <- 0.05

qchisq ( alpha /2, df =r -k -1)

qchisq ( 1-alpha /2, df =r -k -1)

# Avendo denotato che la distribuzione è normale, possiamo eseguire i seguenti passi trattando la popolazione come normale

# --- STIMA PUNTUALE ---

# VALORE STIMATO PER IL PARAMETRO µ
stimamu <- mean ( D_Italia )
stimamu

# VALORE STIMATO PER IL PARAMETRO σ^2
stimasigma2 <-( length ( D_Italia ) -1) * var ( D_Italia )/ length ( D_Italia )
stimasigma2

# --- INTERVALLI DI CONFIDENZA ---

alpha = 0.05

# µ con σ^2 NOTA

qnorm = qnorm (1 - alpha / 2)
qnorm

mean(D_Italia) - qnorm * sd_Italia/sqrt(n_Italia)
mean(D_Italia) + qnorm * sd_Italia/sqrt(n_Italia)

# µ con σ^2 NON NOTA

qt = qt (1 - alpha /2, df =n_Italia -1)
qt

mean_Italia - qt * sd_Italia / sqrt(n_Italia)
mean_Italia + qt * sd_Italia / sqrt(n_Italia)

# σ^2 con µ NOTA

# Supponiamo mu = 151 (in quanto la media è 151.x)

mu = 151

q1 = qchisq (alpha/2, df=n_Italia)
q2 = qchisq(1-alpha/2, df=n_Italia)
q1
q2

((n_Italia-1)*var(D_Italia) + n_Italia*(mean_Italia-mu)**2)/q2
((n_Italia-1)*var(D_Italia) + n_Italia*(mean_Italia-mu)**2)/q1

# σ^2 con µ NON NOTA

q1 = qchisq ( alpha / 2, df =n_Italia -1)
q2 = qchisq ( 1- alpha / 2, df =n_Italia -1)

(n_Italia-1)*var(D_Italia)/q2
(n_Italia-1)*var(D_Italia)/q1

# --- CONFRONTO TRA DUE POPOLAZIONI (NORMALI) ---

# Verifichiamo che la seconda popolazione sia normale con il test del chiquadro

D_Italia_post_2005 <- as.vector(D_dataset["Italia",-(1:9)])
n_Italia_post_2005 = length(D_Italia_post_2005)

mean_Italia_post_2005 <- mean(D_Italia_post_2005)

sd_Italia_post_2005 <- sd(D_Italia_post_2005)

a <- numeric(4)

for(i in 1:4){
  a[i] <- qnorm(0.2*i, mean=mean_Italia_post_2005, sd=sd_Italia_post_2005)
}

r <- 5

nint <- numeric(r)
nint [1] <- length(which(D_Italia_post_2005 < a [1]) )
for(i in 2:4) {
  nint[i] <- length(which((D_Italia_post_2005 >= a[i-1]) & D_Italia_post_2005 < a[i]))
}
nint[5] <- length ( which ( D_Italia_post_2005 >= a [4]) )

chi2 <- sum ((( nint - n_Italia_post_2005* 0.2) / sqrt ( n_Italia_post_2005* 0.2) ) ^2)
chi2

# r num di elementi per ogni intervallo di frequenze (per ogni cella)
r <-5
# k num di parametri non noti
k <-2
# r - k -1 gradi di libertà
alpha <- 0.05

qchisq ( alpha /2, df =r -k -1)

qchisq ( 1-alpha /2, df =r -k -1)

# chi2 compreso --> ipotesi valida --> popolazione normale. Possiamo procedere col confronto

# --- CONFRONTO ---

# µ1 − µ2 con σ^2(1) e σ^2(2) note

qnorm(1-alpha/2, mean = 0, sd = 1)

n_Italia
n_Italia_post_2005

mean_Italia
mean_Italia_post_2005

sd_Italia

mean_Italia - mean_Italia_post_2005 - qnorm(1-alpha/2, mean = 0, sd = 1) * sqrt(sd_Italia^2/n_Italia + sd_Italia^2/n_Italia_post_2005)
mean_Italia - mean_Italia_post_2005 + qnorm(1-alpha/2, mean = 0, sd = 1) * sqrt(sd_Italia^2/n_Italia + sd_Italia^2/n_Italia_post_2005)

# Intervalli positivi -> indicano che il numero di morti è sicuramente superiore nel primo campione (tutti gli anni)
# con un grado di fiducia del 95%

# --- VERIFICA DELLE IPOTESI ---


mean_Italia_post_2005

D_Italia_pre_2005 <- as.vector(D_dataset["Italia",(1:9)])
mean_Italia_pre_2005 <- mean(D_Italia_pre_2005)
mean_Italia_pre_2005

# ??
# Sapendo che l'Italia ha una media di 151 morti, vogliamo mostrare come dal 2005 in poi la media cali.
# Poniamo quindi mu = media_totale = 151 >= mu0 = media_post_2005 = 140
# ??

# Nel 2005 Giovanni emana una legge antifumo anche se lui fuma come un Turco.
# Giovanni assicura al popolo italiano che grazie a questa legge i morti per il fumo passivo saranno minori di 168 (basandosi sul numero di morti precedenti nella media precedente al 2005 (potremmo usare il pre) con una sd di 15 morti)
# L'UE che odia Giovanni e l'Italia vuole verificare se Giovanni dice il vero e non baggianate.
# Quindi negli anni successivi controlla il numero di morti e ottiene un campione dal 2005 al 2018.
# La media ottenuta è salvata in mean_italia_post_2005
# Le variabili saranno:
# mu0 <- 168(valore numerico ipotizzato, valore preso da mean_italia_pre_2005)
# sigma <- 15 (valore numero ipotizzato valore preso da sd_italia)
# n = n_Italia_post_2005
# meancamp = mean_italia_post_2005

# Test unilaterale destro
# H0: mu >= mu_post / H1: mu < mu_post

alpha = 0.05
mu0 <-  168
sigma = 15
qnorm ( alpha , mean =0 , sd =1)
n = n_Italia_post_2005

meancamp = mean_Italia_post_2005

( meancamp - mu0 ) /( sigma / sqrt (n) )

# Stima statistica del test considerato
 stima = ( meancamp - mu0 ) /( sigma / sqrt (n) )
 stima

pvalue <- pnorm ( stima , mean =0 , sd =1)
pvalue

function0()


```

